\chapter{Literature Review}

\section{Early work (2000-2003)}

Bahl and Padmanabhan (2000) proposed RADAR as the first system to utilize RF signals to achieve indoor positioning using a PC. Their research was carried out using a WaveLAN Network Interface Card (NIC) that exposed signal strength and SNR values via its driver interface to the FreeBSD operating system. A Windows based mobile host was set up as the target to be located and it periodically broadcast beacons to the WaveLAN NIC which recorded the signal strength and SNR values while operating as a static base station. A sample data collection step was performed to determine the mean, median and standard deviation of the signal strengths at each sample location. A test sample was then assigned to the location corresponding to the sample value that was nearest to the test sample in the signal space defined by the signal strengths from the 3 base stations present in their experimental setup. They also performed basic analysis of the impact of varying the size of the initial sample database and the impact of user orientation at the time of signal sampling. The major result of their work was the indication that indoor positioning was possible using RF signals and that the indoor positioning problem could be approached in 2 different ways:

\begin{enumerate}
\item as a signal processing problem that aims to compensate for free space path loss and other multipath and obstructive effects by using a propagation model, thus allowing the use of triangulation for positioning 
\item as a nearest neighbor problem for a set of predefined signal samples associated with fixed locations in the target area.
Kotanen, Hannikainen, Leppakoski et al (2003) took forward this research by exploring alternative radio sources and produced an indoor positioning system using Bluetooth devices as their radio source. Their major contribution was the use of Kalman Filters and other signal processing techniques to improve the accuracy of the location estimate. The major limitation of this method for achieving the stated objectives is that Bluetooth capable “anchor points” are not deployed in sufficiently large numbers and a Bluetooth based indoor location technology would require additional hardware deployment and does not have sufficient range to justify the extra cost and complexity of the solution.
\end{enumerate}


\section{Positioning Algorithm Maturation (2004-2006)}
2004 was a year when indoor positioning really started to generate research interest. The primary progress during this period was the analysis of signal characteristics of IEEE 802.11 systems in the 2.4 GHz band as well as the maturation of approaches to positioning based on probabilistic and fuzzy models.
Kaemarungsi and Krishnamurthy (2004)  did a thorough study of the properties exhibited by IEEE 802.11 signals in the 2.4 GHz ISM band as they apply to a real world positioning scenario in this paper. Their major contribution was the statistical analysis of Received Signal Strength datasets. Kaemarungsi and Krishnamurthy first collected data from a single access point to determine the impact of the presence of a user as well as the statistical properties of the dataset (the statistical distribution, the signal stationarity, the variation of the signal at different times of the day etc.). After a thorough analysis, they extended their analysis to datasets generated from multiple access points to determine the degree of independence of signals from the different access points and the similarity of the statistical properties of the access points in a multi-access point scenario. The final part of their work dealt with the differences between RSS fingerprints of two locations to identify commonalities and differences between fingerprints.

RSS values corresponding to different APs show significant clustering behaviour around radio sources and may thus be used with a discriminant to distinguish between locations. Also, the number of distinct tuples for each location are fewer than the number of samples.

These results are important as subsequent systems have failed to take into account one or more of these statistical properties which may be leveraged to improve positioning accuracy. 

Ladd et al (2003, 2005) took up a different approach to the problem – they preferred to use the probabilistic viewpoint and take an approach keeping in mind its practical application in robotics. Their paper, published in 2003 as part of conference proceedings and later in a revised form in 2005 as  by Springer used a fairly advanced probabilistic model to predict the position of a robot. They used a Bayesian Inference Algorithm to generate probability values for motion and position. They also attempted to coarsely estimate the orientation of a user based on the sample data provided to compensate for orientation effects mentioned in . Their model – though fairly complex and quite accurate – suffers from a number of limitations. No effort was made to leverage standard deviation values of the samples. There was no mention of how to evolve and maintain the initial training dataset of signal samples. They also did not practically validate their design in a 3D scenario. Also, specific assumptions regarding Gaussian nature of various signals were made while preparing the probabilistic models but these assumptions don’t hold valid as per the results of .

In 2004, inspired by the methods of Ladd et al, Krum and Horwitz from Microsoft Research attempted to build a motion and location determination system, LOCADIO. Their system aimed to address some of the deficiencies of Ladd’s system. They added interpolation techniques to allow for sparse calibration data, a floor plan and distance dependent set of transition probabilities for location and an explicit probabilistic model for detecting motion. However, the system still requires an explicit training dataset and doesn’t take into sufficient account the orientation of the operator.

In 2006, progress on the orientation front was done by King et al at the University of Mannheim. Their technical report was the first description of any system that utilized digital compasses to determine user orientation. Their probabilistic algorithm utilized Bayes’ Rule and an assumed Gaussian distribution of signal strengths around the reference points to generate location candidates. Orientation information from digital compass measurements was used to limit the dataset to only those samples in the training set that had a similar orientation as the current test datapoint. Simple averaging of the resulting most probable data points was returned to the user as the most likely location. The conditional probabilities used in the system were then updated by Bayes’ rule for each of the reference points. Though novel in their use of orientation information for selecting a subset of the dataset on which to apply probabilistic analysis, this system suffers from a few basic faults. Firstly, as mentioned in , the signal strength distribution is non-Gaussian in the presence of an operator and local deformations occur in the signal strength field. Secondly, the actual orientation information from the digital compass is never used to aid accuracy of the system by including it into the probability distribution function.

Teuber and Eissfeller (2006) approached the positioning problem from the viewpoint of fuzzy logic and they described their results of positioning using a fuzzy logic based system in a large indoor environment (an abandoned aircraft hangar) with Line of Sight between the different APs as well as an office environment. Their results did not show any significant improvement in the office environment scenario compared to prior work but they did have an improvement in the positioning for the LoS case.

\section{Zero Configuration Systems (2007-2010)}

Over the course of 2007 and 2008, research focus shifted away from optimizing the core positioning algorithm for greater accuracy. The major hurdle in largescale deployment of such systems was the requirement to train the system with the location fingerprints a priori. 

Philip Bolliger’s RedPin system  was one of the first few systems that took indoor positioning onto mobile handsets. The system was implemented as a client server application running on a Nokia N95 mobile handset which was communicating with a Java + MySQL based locator application on a server. It was also one of the first systems to successfully demonstrate some degree of sensor fusion across Bluetooth, GSM and WiFi. However, the algorithms used for positioning were very simple (using a simple Boolean distance measure) and the accuracy requirements were not stringent. However, one of the big contributions of the system was the idea that a fingerprint pattern can be developed over time with increasing accuracy if a sufficient number of people are using the system. This led to the idea of Zero Configuration Systems that can be used to bootstrap positioning in an unknown building given just a map and a few initial location fingerprints.

During the same period, progress was being made on the signal processing approach to indoor positioning by Lim et al by determining how many measurements are required to sufficiently determine the parameter n in the path loss equation when deploying an IEEE 802.11 based network. They’ve utilized Singular Value Decomposition (SVD) based methods to generate a distance estimate from the path loss equation and requires just a single measurement from co-located access points to bootstrap the system. This method, however, does not take into account orientation and uses a path loss equation for the RSS distribution which is incorrect as per the statistical investigations in .

Barry, Fisher and Chang (2009)  made a beautiful study of the way a collaborative, user supplied training data system can provide progressively increasing accuracy. Their system was operating with 200 untrained users spanning 5 buildings over a period of 1 year wherein it was able to perform over 1,000,000 localizations with over 8,700 user generated training datapoints. In over 94\% cases, they were able to localize to within 10m. This was a major advancement. The major ideas presented in the paper were the way that users were incentivized to contribute to the system’s fingerprint location and the performance of the system was measured in terms of coverage and accuracy. However, the accuracy results were quite poor as compared to King’s results.

MIT CSAIL and Nokia Research Labs have also taken up this line of research. Their paper “Growing an Organic Indoor Location System” published in 2010 describes how crowdsourcing techniques similar to the ones described in  can be used to generate the training dataset. They have also used clustering techniques to filter out outliers from the training dataset. Crowdsourcing ensures that the system will evolve with changing environmental conditions. The authors Park et al have used probabilistic techniques to estimate accuracy and have fixed empirical thresholds on spatial uncertainties to govern if the system will prompt the user to provide a location estimate. However, their entire paper doesn’t have a single mention of orientation information being used for determining location. 

\section{Shortcomings and Research Gaps \label{sec:shortcomings} }

\typeout{Do \ref{sec:shortcomings} }

\section{Summary \label{sec:litsummary}}

\typeout{Do \ref{sec:litsummary}}